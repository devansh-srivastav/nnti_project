{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Language model inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal if this first task is to familiarize yourself with the huggingface transformers and dataset libraries. You will learn how to load and tokenize a dataset, how to load a pre-trained language model, and finally, how to run a model in inference mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to complete the missing code blocks below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset, load_dataset_builder, get_dataset_split_names, get_dataset_config_names\n",
    "from transformers import XGLMTokenizer, XGLMTokenizerFast, XGLMForCausalLM, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET_NAME = \"facebook/flores\" # specify dataset name\n",
    "MODEL_NAME = \"facebook/xglm-564M\" # specify model name\n",
    "# MODEL_NAME = \"gpt2\" # specify model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The creation of FLORES-200 doubles the existing language coverage of FLORES-101. \n",
      "Given the nature of the new languages, which have less standardization and require \n",
      "more specialized professional translations, the verification process became more complex. \n",
      "This required modifications to the translation workflow. FLORES-200 has several languages \n",
      "which were not translated from English. Specifically, several languages were translated \n",
      "from Spanish, French, Russian and Modern Standard Arabic. Moreover, FLORES-200 also \n",
      "includes two script alternatives for four languages. FLORES-200 consists of translations \n",
      "from 842 distinct web articles, totaling 3001 sentences. These sentences are divided \n",
      "into three splits: dev, devtest, and test (hidden). On average, sentences are approximately \n",
      "21 words long.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore a dataset\n",
    "\n",
    "# covered language codes can be found here: https://github.com/openlanguagedata/flores?tab=readme-ov-file#language-coverage\n",
    "\n",
    "ds_builder = load_dataset_builder(\"facebook/flores\", \"deu_Latn\", trust_remote_code=True)\n",
    "print(ds_builder.info.description) # print the dataset description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:\n",
      "id\n",
      "URL\n",
      "domain\n",
      "topic\n",
      "has_image\n",
      "has_hyperlink\n",
      "sentence\n"
     ]
    }
   ],
   "source": [
    "# print the features (columns) of the dataset\n",
    "# TODO: your code goes here\n",
    "print(\"Features:\")\n",
    "for features in ds_builder.info.features:\n",
    "    print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available splits:\n",
      "dev\n",
      "devtest\n"
     ]
    }
   ],
   "source": [
    "# get the available splits\n",
    "# TODO: your code goes here\n",
    "print(\"Available splits:\")\n",
    "for split in ds_builder.info.splits:\n",
    "    print(split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data, tokenize, and batchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify languages\n",
    "LANGUAGES = [\n",
    "    \"eng_Latn\",\n",
    "    \"spa_Latn\",\n",
    "    \"ita_Latn\",\n",
    "    \"deu_Latn\",\n",
    "    \"arb_Arab\",\n",
    "    \"tel_Telu\",\n",
    "    \"tam_Taml\",\n",
    "    \"quy_Latn\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load flores data for each language\n",
    "# TODO: your code goes here\n",
    "data = {}\n",
    "for lang in LANGUAGES:\n",
    "    data[lang] = load_dataset(\"facebook/flores\", lang, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    dev: Dataset({\n",
      "        features: ['id', 'URL', 'domain', 'topic', 'has_image', 'has_hyperlink', 'sentence'],\n",
      "        num_rows: 997\n",
      "    })\n",
      "    devtest: Dataset({\n",
      "        features: ['id', 'URL', 'domain', 'topic', 'has_image', 'has_hyperlink', 'sentence'],\n",
      "        num_rows: 1012\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# let's look at the English subset\n",
    "# TODO: your code goes here\n",
    "print(data[\"eng_Latn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 1\n",
      "URL: https://en.wikinews.org/wiki/Scientists_say_new_medical_diagnostic_chip_can_sort_cells_anywhere_with_an_inkjet\n",
      "domain: wikinews\n",
      "topic: health\n",
      "has_image: 0\n",
      "has_hyperlink: 0\n",
      "sentence: On Monday, scientists from the Stanford University School of Medicine announced the invention of a new diagnostic tool that can sort cells by type: a tiny printable chip that can be manufactured using standard inkjet printers for possibly about one U.S. cent each.\n"
     ]
    }
   ],
   "source": [
    "# let's look at an individal sample from the dataset\n",
    "# TODO: your code goes here\n",
    "for feature, value in data[\"eng_Latn\"][\"dev\"][0].items():\n",
    "    print(f\"{feature}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a53f04807b450a90a8c5bb3d2fec6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize the data\n",
    "\n",
    "# load a pre-trained tokenizer from the huggingface hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# gpt2 does not have a padding token, so we have to add it manually\n",
    "if MODEL_NAME == \"gpt2\":\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.unk_token})\n",
    "\n",
    "# specify the tokenization function\n",
    "def tokenization(example):\n",
    "    # fill in here\n",
    "    tokenized = tokenizer(example[\"sentence\"], return_tensors=\"pt\", padding='max_length', truncation=True)\n",
    "    return tokenized\n",
    "    pass\n",
    "\n",
    "# TODO: your code goes here\n",
    "tokenized_data = {}\n",
    "for lang in LANGUAGES:\n",
    "    tokenized_data[lang] = data[lang].map(tokenization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 1\n",
      "URL: https://en.wikinews.org/wiki/Scientists_say_new_medical_diagnostic_chip_can_sort_cells_anywhere_with_an_inkjet\n",
      "domain: wikinews\n",
      "topic: health\n",
      "has_image: 0\n",
      "has_hyperlink: 0\n",
      "sentence: On Monday, scientists from the Stanford University School of Medicine announced the invention of a new diagnostic tool that can sort cells by type: a tiny printable chip that can be manufactured using standard inkjet printers for possibly about one U.S. cent each.\n",
      "input_ids: [[2, 1504, 28488, 4, 140003, 501, 32, 200884, 6073, 9512, 48, 88230, 76168, 32, 160597, 48, 11, 929, 55516, 35761, 155, 490, 9482, 89288, 235, 6950, 13, 11, 61368, 24049, 2005, 37295, 155, 490, 113, 213481, 72, 1117, 5885, 86368, 6929, 111288, 7, 73, 59298, 769, 743, 242, 5, 211, 5, 19015, 5129, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# let's take a look at a tokenized sample\n",
    "# TODO: your code goes here\n",
    "for feature, value in tokenized_data[\"eng_Latn\"][\"dev\"][0].items():\n",
    "    print(f\"{feature}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to filter the keys\n",
    "def filter_keys(entry):\n",
    "    return {key: entry[key] for key in ['input_ids', 'attention_mask']}\n",
    "\n",
    "\n",
    "filtered_data_dev = {}\n",
    "filtered_data_devtest = {}\n",
    "\n",
    "# Iterate over each element in tokenized_data[lang][\"dev\"] and filter the keys\n",
    "for lang in LANGUAGES:\n",
    "    filtered_data_dev[lang] = [filter_keys(entry) for entry in tokenized_data[lang][\"dev\"]]\n",
    "    filtered_data_devtest[lang] = [filter_keys(entry) for entry in tokenized_data[lang][\"devtest\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [[2, 1504, 28488, 4, 140003, 501, 32, 200884, 6073, 9512, 48, 88230, 76168, 32, 160597, 48, 11, 929, 55516, 35761, 155, 490, 9482, 89288, 235, 6950, 13, 11, 61368, 24049, 2005, 37295, 155, 490, 113, 213481, 72, 1117, 5885, 86368, 6929, 111288, 7, 73, 59298, 769, 743, 242, 5, 211, 5, 19015, 5129, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "for feature, value in filtered_data_dev[\"eng_Latn\"][0].items():\n",
    "    print(f\"{feature}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a pytorch data loader for each dataset\n",
    "BATCH_SIZE = 2 # for testing purposes, we start with a batch size of 2. You can change this later.\n",
    "\n",
    "# TODO: your code goes here\n",
    "data_loader_dev = {}\n",
    "data_loader_devtest = {}\n",
    "for lang in LANGUAGES:\n",
    "    data_loader_dev[lang] = torch.utils.data.DataLoader(filtered_data_dev[lang], batch_size=BATCH_SIZE)\n",
    "    data_loader_devtest[lang] = torch.utils.data.DataLoader(filtered_data_devtest[lang], batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XGLMForCausalLM were not initialized from the model checkpoint at facebook/xglm-564M and are newly initialized: ['model.embed_positions.weights']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGLMForCausalLM(\n",
       "  (model): XGLMModel(\n",
       "    (embed_tokens): Embedding(256008, 1024, padding_idx=1)\n",
       "    (embed_positions): XGLMSinusoidalPositionalEmbedding()\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (activation_fn): GELUActivation()\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=256008, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pre-trained model from the huggingface hub\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "# put the model into evaluation mode\n",
    "# TODO: your code goes here\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed loss computation for eng_Latn\n",
      "Average loss for eng_Latn: 7.845671591156709\n",
      "Completed loss computation for spa_Latn\n",
      "Average loss for spa_Latn: 8.024704102762716\n",
      "Completed loss computation for ita_Latn\n",
      "Average loss for ita_Latn: 8.422627958362709\n",
      "Completed loss computation for deu_Latn\n",
      "Average loss for deu_Latn: 8.448769486738827\n",
      "Completed loss computation for arb_Arab\n",
      "Average loss for arb_Arab: 8.464308300094757\n",
      "Completed loss computation for tel_Telu\n",
      "Average loss for tel_Telu: 8.385566569043544\n",
      "Completed loss computation for tam_Taml\n",
      "Average loss for tam_Taml: 8.182438421822742\n",
      "Completed loss computation for quy_Latn\n",
      "Average loss for quy_Latn: 8.568757355332613\n"
     ]
    }
   ],
   "source": [
    "losses = {lang: [] for lang in LANGUAGES} # store per-batch losses for each language\n",
    "\n",
    "# iterate over the dataset for each language and compute the cross-entropy loss per batch \n",
    "for lang in LANGUAGES:\n",
    "    for batch in data_loader_dev[lang]:\n",
    "        # Extract input IDs and attention masks\n",
    "        input_ids = torch.stack([torch.cat(seq) for seq in batch['input_ids']]).to(device)\n",
    "        attention_mask = torch.stack([torch.cat(seq) for seq in batch['attention_mask']]).to(device)\n",
    "        \n",
    "        # Shift input IDs to create target sequence\n",
    "        target_ids = input_ids[:, 1:].contiguous()  # Shift by one token\n",
    "        target_mask = attention_mask[:, 1:].contiguous()\n",
    "        \n",
    "        # Compute model outputs\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Get logits for next word prediction\n",
    "        logits = outputs.logits[:, :-1].contiguous()  # Discard the last token's logits\n",
    "        \n",
    "        # Flatten the logits and target sequences\n",
    "        logits_flat = logits.view(-1, logits.size(-1))\n",
    "        target_ids_flat = target_ids.view(-1)\n",
    "        target_mask_flat = target_mask.view(-1)\n",
    "        \n",
    "        # Filter out masked positions (where target_mask_flat == 0)\n",
    "        masked_indices = target_mask_flat.nonzero().squeeze()\n",
    "        logits_flat_masked = logits_flat.index_select(0, masked_indices)\n",
    "        target_ids_flat_masked = target_ids_flat.index_select(0, masked_indices)\n",
    "        \n",
    "        # Compute cross-entropy loss\n",
    "        loss = torch.nn.functional.cross_entropy(logits_flat_masked, target_ids_flat_masked)\n",
    "        \n",
    "        # Append loss to the list for the current language\n",
    "        losses[lang].append(loss.item())\n",
    "    print(f\"Completed loss computation for {lang}\")\n",
    "    print(f\"Average loss for {lang}: {np.mean(losses[lang])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize loss per language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqYAAAHUCAYAAADoeerIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABDzElEQVR4nO3de5xNdf///+c2ZvachxnjPGZEjpFEOeYUcuijruorEpLrSpTQJSTp5FAiXSmV5JBCkS5XDqGik/MhlfNxJheRmGFiGPP6/dHPvtpmjDHGrCWP++22bqz3eu+1Xvu991rznLXX2uMxMxMAAADgsAJOFwAAAABIBFMAAAC4BMEUAAAArkAwBQAAgCsQTAEAAOAKBFMAAAC4AsEUAAAArkAwBQAAgCsQTAEAAOAKBFMAl2TmzJmqWrWqQkJC5PF4tGHDBkfq8Hg8euSRR/J9u3v27JHH49HLL7+c48fUrFlTjz322GWpZ+nSpfJ4PFq6dOllWf+5PB5PltPIkSOz7P/vf/9bjRo1UmRkpMLCwlS1alW9/fbb513/iRMnVKFChSzH+Oxz9Xg8mjx5cpaPb9q0qTwejxISEnL7FAHko4JOFwDgynXo0CHdf//9uu222/TGG2/I6/WqQoUKTpflart379b69es1duxYp0vJM3fffbcef/xxv7YyZcpk6jdy5EgNHjxYPXr00KBBgxQYGKgtW7bo1KlT5133kCFDlJqamu32IyIiNHHiRHXt2tWvfffu3Vq6dKkiIyNz/mQAOIpgCiDXtm3bptOnT6tTp05q1KiR0+VcEWbNmqWiRYuqQYMGTpeSZ4oVK6Y6depk22ft2rUaPHiwRowYoSeeeMLX3qxZs/M+ZtWqVXrttdf0/vvv65577jlvv/bt2+udd97R9u3bde211/ra3333XZUqVUrVqlXTpk2bLuIZAXAKH+UDyJWuXbv6wlX79u3l8XjUuHFjrVmzRvfee68SEhIUEhKihIQEdejQQXv37s20jn379ukf//iH4uLiFBQUpJIlS+ruu+/WL7/84uuTkpKif/7znypbtqyCgoJUqlQp9enT57xn0d566y1VqFBBXq9XVapU0YwZMzL1+fHHH9WuXTsVLlxYwcHBqlGjhqZMmZKpX2Jiojp16qSiRYvK6/WqcuXKGj16tDIyMrIdm9OnT6tLly4KDw/Xp59+6rds9uzZuvPOO1WgQAFt375dkZGRmULXF198oYCAAA0ZMsTXlpaWpscff1zFixdXaGiobrnlFq1du1YJCQmZzhSeq2vXrgoPD9eWLVvUsmVLhYWFqUSJEr6P21esWKEGDRooLCxMFSpUyHIsLtW4cePk9Xr16KOP5qj/qVOn1K1bN/Xq1Uu1atXKtm/z5s0VFxend99919eWkZGhKVOmqEuXLipQgB91wBXDACAXduzYYa+//rpJsuHDh9vy5cvtp59+so8++siefvppmzNnji1btsxmzJhhjRo1stjYWDt06JDv8T///LOVKFHCihQpYmPGjLElS5bYzJkzrVu3brZ582YzM0tNTbUaNWr49Xn11VctKirKmjZtahkZGb71SbK4uDirUqWKTZ8+3ebOnWu33XabSbKPPvrI12/Lli0WERFh5cqVs6lTp9q8efOsQ4cOJslefPFFX7+DBw9aqVKlLDY21t58801buHChPfLIIybJHn74YV+/3bt3myQbNWqUmZkdOXLEmjRpYsWLF7c1a9b4jVlSUpJ5PB5btGiRr23GjBkmyV599VUzM9u/f78VK1bMGjVqZOnp6b5+HTp0sAIFCtjAgQNt0aJFNnbsWIuLi7OoqCjr0qWLr9+XX35pkuzLL7/0tXXp0sWCgoKscuXK9uqrr9rixYvtgQceMEk2aNAgq1Chgk2cONE+++wza9u2rUnKVPv5SLLChQtbcHCwBQUFWc2aNe3dd9/N1O+aa66xmjVr2nvvvWcVKlSwAgUKWKlSpWzAgAGWlpaWqf/gwYMtISHBjh8/nmmMz32uH330kQ0ZMsRKlizpG7MFCxaYx+OxHTt2WJs2bSw+Pj5HzweAswimAHLtz8HgfNLT0+348eMWFhbmC19mZt26dbPAwEDbtGnTeR87YsQIK1CggK1evdqvfdasWSbJ5s+f72uTZCEhIXbgwAG/bVeqVMnKly/va7v33nvN6/VaYmKi3zpbtWploaGhdvToUTMzGzhwoEmylStX+vV7+OGHzePx2NatW83MP5ju3r3bqlSpYlWqVLE9e/Zkej5jx461woUL2+nTpzOtMygoyJYvX25Nmza1okWL2n//+1/f8p9++skk2YABA/weN336dJOUo2AqyWbPnu1rO336tMXGxpokW7duna/98OHDFhAQYP369ctUf1Y6duxo77//vn311Vc2a9Ysa9WqlUmyp556yq+f1+u1iIgIK1y4sI0bN86++OILGzx4sAUEBFjHjh39+q5fv94CAwNt4cKFZpY5/J/7XD/66CPbtWuXeTwe+/TTT83M7J577rHGjRubmRFMgSsIwRRArmUVTI8dO2ZPPPGElStXzgICAkySb+rRo4evX4kSJaxFixbZrr9+/fpWvXp1O336tN907Ngx83g89sQTT/j6SrK2bdtmWsfQoUNNkiUlJZmZWdGiRa1169aZ+s2cOdMk2YIFC8zM7KabbrIqVapk6rdy5UqTZOPHjzez/4WmDh06WLFixaxJkyZ25MiRLJ9Pw4YNrWvXrpnaT548aTfccIMFBwdbgQIF/M6ompm98cYbJsnWrl3r13769GkrWLBgjoKpx+OxEydO+D2+bt26VqJEiUz1lChRwu666y6/7fx5+vOZ6qy0bdvWChYsaAcPHvS1BQYGmiSbPn26X98+ffqYJNu+fbtvWzfccIN16tTJ1ycnwdTMrEmTJva3v/3Nfv31VwsKCrKpU6eaGcEUuJJw4Q2APNWxY0eNGzdO3bt312effaZVq1Zp9erVio2N1YkTJ3z9Dh06pNKlS2e7rl9++UUbN25UYGCg3xQRESEz06+//urXv3jx4pnWcbbt8OHDvn9LlCiRqV/JkiVz1e+sxYsX65dfflH37t1VqFChTI87cOCAvv32W911112Zlnm9XnXs2FEnT55UjRo11Lx5c7/lZ7dVrFgxv/aCBQsqJiYm0/qyEhoaquDgYL+2oKAgRUdHZ+obFBSkkydP+ubPHf8LXYPaqVMnpaena82aNb62s3W2bNnSr2+rVq0kSevWrZMkjR07Vrt27dLQoUN19OhRHT16VCkpKZKkkydP6ujRozpz5kyW233wwQf1n//8R2PGjFFISIjuvvvubOsE4D7clQ8gzyQnJ+vTTz/V0KFDNXDgQF97WlqafvvtN7++sbGx+vnnn7NdX5EiRRQSEuJ3U8u5y//swIEDmfqcbTsbjGJiYrR///5M/f773//6rTOn/c7q37+/du7cqc6dOys9PV2dO3f2Wz5nzhyFhYVlCp3SHzdjPf3006pdu7ZWr16tMWPGqF+/fr7lZ2v/5ZdfVKpUKV97enp6poB8OaxevdpvvmzZstn2NzNJ8rvpqHr16lm+Puf2/fHHH5WcnOx3d/1ZQ4YM0ZAhQ7R+/XrVqFEj0/K//e1v6tWrl0aOHKm///3vCgkJyf6JAXAdgimAPOPxeGRm8nq9fu3vvPNOprNcrVq10nvvvaetW7eqYsWKWa6vbdu2Gj58uGJiYi4YhiTp888/1y+//OI7s3jmzBnNnDlT5cqV852dbdasmebMmaP//ve/vrOfkjR16lSFhob6vvaoWbNmGjFihNatW6eaNWv69fN4PGrSpInftgsUKKC33npL4eHh6tq1q1JTU/Xwww/7ls+ePVtt27bNNDapqam65557lJCQoC+//FIDBw7UwIEDVb9+fd18882SpFtuuUXSH3/M4M+1zJo1S+np6Rccl0t1obviz/Xee+8pMDBQN954o6/trrvu0qJFi7RgwQJ17NjR1z5//nwVKFBAtWvXliQNHDgw07cMHDhwQB06dFCPHj3Uvn17lS9fPsvthoSE6Omnn9ZXX33lN/YArhwEUwB5JjIyUrfccotGjRqlIkWKKCEhQcuWLdPEiRMzfbz93HPPacGCBbrlllv05JNPqlq1ajp69KgWLlyofv36qVKlSurTp49mz56tW265RX379lX16tWVkZGhxMRELVq0SI8//rgvvEl/nMVs2rSphgwZorCwML3xxhvasmWL31dGDR06VJ9++qmaNGmip59+WtHR0Xr//fc1b948vfTSS4qKipIk9e3bV1OnTlWbNm303HPPKT4+XvPmzdMbb7yhhx9++Lx/SGD06NGKiIhQz549dfz4cfXv31+HDx/WsmXLsvzqqh49eigxMVGrVq1SWFiYRo8ereXLl+vee+/V+vXrVahQIVWtWlUdOnTQ6NGjFRAQoKZNm+qnn37S6NGjFRUV5djXIY0aNUqbNm1Ss2bNVLp0aR08eFATJ07UokWL9Mwzz/idVX7ggQf01ltvqWfPnvr1119VpUoVLVmyRK+//rp69uyp+Ph4SVKlSpVUqVIlv+3s2bNHklSuXDk1btw425r69evnd7YZwBXG2UtcAVzJsrr56eeff7a77rrLChcubBEREXbbbbfZjz/+aPHx8X436Zj98fVJ3bp1s+LFi1tgYKCVLFnS/t//+3/2yy+/+PocP37cnnrqKatYsaIFBQVZVFSUVatWzfr27et3B74k69Wrl73xxhtWrlw5CwwMtEqVKtn777+fqe4ffvjBbr/9douKirKgoCC7/vrrbdKkSZn67d271zp27GgxMTEWGBhoFStWtFGjRtmZM2d8fc53Y86oUaNMkj399NP2zjvvWGhoqKWmpvr1mTBhgknKtO0dO3ZYZGSk3XHHHb62kydPWr9+/axo0aIWHBxsderUseXLl1tUVJT17ds302ty7s1PYWFhmZ5fo0aNrGrVqpna4+PjrU2bNpnazzV37lxr0KCBxcbGWsGCBS0iIsIaNmyY6Qansw4fPmwPPfSQFStWzAIDA61ChQqZxjMrOb356Xy4+Qm4cnjM/v8LfAAAl0Xr1q0VEhKi2bNn5+l6v/vuO9WvX1/vv/++38fjAHClIpgCwBVg8eLFWr58uW688UaFhITo+++/18iRIxUVFaWNGzdmuuMeAK5EXGMKAFeAyMhILVq0SGPHjtWxY8dUpEgRtWrVSiNGjCCUAvjL4IwpAAAAXMHRL9g/duyY+vTpo/j4eIWEhKhevXqZvi8PAAAAVwdHg2n37t21ePFivffee/rhhx/UokUL3Xrrrdq3b5+TZQEAAMABjn2Uf+LECUVEROjf//632rRp42uvUaOG2rZtqxdeeMGJsgAAAOAQx25+Sk9P15kzZzJdtB8SEqJvvvkmy8ekpaUpLS3NN5+RkaHffvtNMTEx8ng8l7VeAAAAXDwz07Fjx1SyZMkL/0EQx75B1czq1q1rjRo1sn379ll6erq999575vF4rEKFCln2Hzp0qEliYmJiYmJiYmK6wqakpKQLZkNH78rfuXOnunXrpq+++koBAQGqWbOmKlSooHXr1mnTpk2Z+p97xjQ5OVllypRRUlKSIiMj87N0AAAA5EBKSori4uJ09OhR3599Ph9Hv8e0XLlyWrZsmVJTU5WSkqISJUqoffv2Klu2bJb9vV6vvF5vpvbIyEiCKQAAgIvl5LJLR+/KPyssLEwlSpTQkSNH9Nlnn6ldu3ZOlwQAAIB85ugZ088++0xmpooVK2rHjh3q37+/KlasqAceeMDJsgAAAOAAR8+YJicnq1evXqpUqZI6d+6sBg0aaNGiRQoMDHSyLAAAADjgiv6TpCkpKYqKilJycjLXmAIAALjQxeQ1V1xjCgAAABBMAQAA4AoEUwAAALgCwRQAAACuQDAFAACAKxBMAQAA4AoEUwAAALgCwRQAAACuQDAFAACAKxBMAQAA4AoEUwAAALhCQacLAAAAcItnnnnG6RLyhVufJ2dMAQAA4AoEUwAAALgCwRQAAACuwDWmAHAFeb3HF06XkC96vdnU6RIAOIBgCsBVNleq7HQJ+aLyls1OlwAArkMwBfJZtSnVnC4hX/zQ5QenSwAAXGG4xhQAAACuwBlTAMBfyuj2bZ0u4bJ7fOanTpcAXBacMQUAAIArEEwBAADgCgRTAAAAuALBFAAAAK5AMAUAAIArEEwBAADgCgRTAAAAuALBFAAAAK7AF+wDAHAV+Xng106XkC9Kj2zodAnIBc6YAgAAwBUIpgAAAHAFgikAAABcwdFgmp6erqeeekply5ZVSEiIrrnmGj333HPKyMhwsiwAAAA4wNGbn1588UW9+eabmjJliqpWrao1a9bogQceUFRUlB577DEnSwMAAEA+czSYLl++XO3atVObNm0kSQkJCZo+fbrWrFmTZf+0tDSlpaX55lNSUvKlTlykZ6KcruDyeybZ6QoAAPjLcfSj/AYNGujzzz/Xtm3bJEnff/+9vvnmG7Vu3TrL/iNGjFBUVJRviouLy89yAQAAcBk5esZ0wIABSk5OVqVKlRQQEKAzZ85o2LBh6tChQ5b9Bw0apH79+vnmU1JSCKcAAAB/EY4G05kzZ2ratGn64IMPVLVqVW3YsEF9+vRRyZIl1aVLl0z9vV6vvF6vA5UCAADgcnM0mPbv318DBw7UvffeK0mqVq2a9u7dqxEjRmQZTAEAAPDX5eg1pr///rsKFPAvISAggK+LAgAAuAo5esb09ttv17Bhw1SmTBlVrVpV69ev15gxY9StWzcnywIAAIADHA2mr732moYMGaKePXvq4MGDKlmypB566CE9/fTTTpYFAAAABzgaTCMiIjR27FiNHTvWyTIuSsLAeU6XkC/2jGzjdAkAAOAq4+g1pgAAAMBZBFMAAAC4AsEUAAAArkAwBQAAgCsQTAEAAOAKBFMAAAC4AsEUAAAArkAwBQAAgCsQTAEAAOAKBFMAAAC4AsEUAAAArkAwBQAAgCsQTAEAAOAKBFMAAAC4AsEUAAAArkAwBQAAgCsQTAEAAOAKBFMAAAC4AsEUAAAArkAwBQAAgCsQTAEAAOAKBFMAAAC4AsEUAAAArkAwBQAAgCsQTAEAAOAKBFMAAAC4AsEUAAAArkAwBQAAgCsQTAEAAOAKBFMAAAC4AsEUAAAAruBoME1ISJDH48k09erVy8myAAAA4ICCTm589erVOnPmjG/+xx9/VPPmzXXPPfc4WBUAAACc4GgwjY2N9ZsfOXKkypUrp0aNGjlUEQAAAJziaDD9s1OnTmnatGnq16+fPB5Pln3S0tKUlpbmm09JScmv8gAAAHCZuebmp08++URHjx5V165dz9tnxIgRioqK8k1xcXH5VyAAAAAuK9cE04kTJ6pVq1YqWbLkefsMGjRIycnJvikpKSkfKwQAAMDl5IqP8vfu3aslS5bo448/zraf1+uV1+vNp6oAAACQn1xxxnTSpEkqWrSo2rRp43QpAAAAcIjjwTQjI0OTJk1Sly5dVLCgK07gAgAAwAGOB9MlS5YoMTFR3bp1c7oUAAAAOMjxU5QtWrSQmTldBgAAABzm+BlTAAAAQCKYAgAAwCUIpgAAAHAFgikAAABcgWAKAAAAVyCYAgAAwBUIpgAAAHAFgikAAABcgWAKAAAAVyCYAgAAwBUIpgAAAHAFgikAAABcgWAKAAAAVyCYAgAAwBUIpgAAAHAFgikAAABcgWAKAAAAVyCYAgAAwBUIpgAAAHAFgikAAABcgWAKAAAAVyCYAgAAwBUIpgAAAHAFgikAAABcgWAKAAAAVyCYAgAAwBUIpgAAAHAFgikAAABcgWAKAAAAVyCYAgAAwBUIpgAAAHAFgikAAABcwfFgum/fPnXq1EkxMTEKDQ1VjRo1tHbtWqfLAgAAQD4r6OTGjxw5ovr166tJkyZasGCBihYtqp07d6pQoUJOlgUAAAAHOBpMX3zxRcXFxWnSpEm+toSEBOcKAgAAgGMc/Sh/7ty5qlWrlu655x4VLVpUN9xwgyZMmHDe/mlpaUpJSfGbAAAA8NfgaDDdtWuXxo8fr2uvvVafffaZevTood69e2vq1KlZ9h8xYoSioqJ8U1xcXD5XDAAAgMvF0WCakZGhmjVravjw4brhhhv00EMP6e9//7vGjx+fZf9BgwYpOTnZNyUlJeVzxQAAALhcHA2mJUqUUJUqVfzaKleurMTExCz7e71eRUZG+k0AAAD4a3A0mNavX19bt271a9u2bZvi4+MdqggAAABOcTSY9u3bVytWrNDw4cO1Y8cOffDBB3r77bfVq1cvJ8sCAACAAxwNprVr19acOXM0ffp0XXfddXr++ec1duxY3XfffU6WBQAAAAc4+j2mktS2bVu1bdvW6TIAAADgMMf/JCkAAAAgEUwBAADgEgRTAAAAuALBFAAAAK5AMAUAAIArEEwBAADgCgRTAAAAuALBFAAAAK5AMAUAAIArEEwBAADgCgRTAAAAuALBFAAAAK5AMAUAAIArEEwBAADgCgRTAAAAuALBFAAAAK5AMAUAAIArEEwBAADgCgRTAAAAuALBFAAAAK5AMAUAAIArEEwBAADgCgRTAAAAuALBFAAAAK5AMAUAAIArEEwBAADgCgRTAAAAuALBFAAAAK5AMAUAAIArEEwBAADgCgRTAAAAuALBFAAAAK7gaDB95pln5PF4/KbixYs7WRIAAAAcUtDpAqpWraolS5b45gMCAhysBgAAAE5xPJgWLFiQs6QAAABw/hrT7du3q2TJkipbtqzuvfde7dq167x909LSlJKS4jcBAADgr8HRYHrzzTdr6tSp+uyzzzRhwgQdOHBA9erV0+HDh7PsP2LECEVFRfmmuLi4fK4YAAAAl4ujwbRVq1a66667VK1aNd16662aN2+eJGnKlClZ9h80aJCSk5N9U1JSUn6WCwAAgMvI8WtM/ywsLEzVqlXT9u3bs1zu9Xrl9XrzuSoAAADkB8evMf2ztLQ0bd68WSVKlHC6FAAAAOQzR4PpP//5Ty1btky7d+/WypUrdffddyslJUVdunRxsiwAAAA4wNGP8n/++Wd16NBBv/76q2JjY1WnTh2tWLFC8fHxTpYFAAAABzgaTGfMmOHk5gEAAOAirrrGFAAAAFcvgikAAABcgWAKAAAAVyCYAgAAwBUIpgAAAHCFXAXTKVOm+P58qCQ98cQTKlSokOrVq6e9e/fmWXEAAAC4euQqmA4fPlwhISGSpOXLl2vcuHF66aWXVKRIEfXt2zdPCwQAAMDVIVffY5qUlKTy5ctLkj755BPdfffd+sc//qH69eurcePGeVkfAAAArhK5OmMaHh6uw4cPS5IWLVqkW2+9VZIUHBysEydO5F11AAAAuGrk6oxp8+bN1b17d91www3atm2b2rRpI0n66aeflJCQkJf1AQAA4CqRqzOmr7/+uurWratDhw5p9uzZiomJkSStXbtWHTp0yNMCAQAAcHXI1RnTQoUKady4cZnan3322UsuCAAAAFenXJ0xXbhwob755hvf/Ouvv64aNWqoY8eOOnLkSJ4VBwAAgKtHroJp//79lZKSIkn64Ycf9Pjjj6t169batWuX+vXrl6cFAgAA4OqQq4/yd+/erSpVqkiSZs+erbZt22r48OFat26dWrdunacFAgAA4OqQqzOmQUFB+v333yVJS5YsUYsWLSRJ0dHRvjOpAAAAwMXI1RnTBg0aqF+/fqpfv75WrVqlmTNnSpK2bdum0qVL52mBAAAAuDrk6ozpuHHjVLBgQc2aNUvjx49XqVKlJEkLFizQbbfdlqcFAgAA4OqQqzOmZcqU0aeffpqp/ZVXXrnkggAAAHB1ylUwlaQzZ87ok08+0ebNm+XxeFS5cmW1a9dOAQEBeVkfAAAArhK5CqY7duxQ69attW/fPlWsWFFmpm3btikuLk7z5s1TuXLl8rpOAAAA/MXl6hrT3r17q1y5ckpKStK6deu0fv16JSYmqmzZsurdu3de1wgAAICrQK7OmC5btkwrVqxQdHS0ry0mJkYjR45U/fr186w4AAAAXD1ydcbU6/Xq2LFjmdqPHz+uoKCgSy4KAAAAV59cBdO2bdvqH//4h1auXCkzk5lpxYoV6tGjh/7v//4vr2sEAADAVSBXwfRf//qXypUrp7p16yo4OFjBwcGqV6+eypcvr7Fjx+ZxiQAAALga5Ooa00KFCunf//63duzYoc2bN8vMVKVKFZUvXz6v6wMAAMBVIsfBtF+/ftkuX7p0qe//Y8aMyXVBAAAAuDrlOJiuX78+R/08Hk+uiwEAAMDVK8fB9Msvv7ycdQAAAOAql6ubnwAAAIC8RjAFAACAK7gmmI4YMUIej0d9+vRxuhQAAAA4wBXBdPXq1Xr77bdVvXp1p0sBAACAQxwPpsePH9d9992nCRMmqHDhwtn2TUtLU0pKit8EAACAvwbHg2mvXr3Upk0b3XrrrRfsO2LECEVFRfmmuLi4fKgQAAAA+cHRYDpjxgytW7dOI0aMyFH/QYMGKTk52TclJSVd5goBAACQX3L1J0nzQlJSkh577DEtWrRIwcHBOXqM1+uV1+u9zJUBAADACY4F07Vr1+rgwYO68cYbfW1nzpzRV199pXHjxiktLU0BAQFOlQcAAIB85lgwbdasmX744Qe/tgceeECVKlXSgAEDCKUAAABXGceCaUREhK677jq/trCwMMXExGRqBwAAwF+f43flAwAAAJKDZ0yzsnTpUqdLAAAAgEM4YwoAAABXIJgCAADAFQimAAAAcAWCKQAAAFyBYAoAAABXIJgCAADAFQimAAAAcAWCKQAAAFyBYAoAAABXIJgCAADAFQimAAAAcAWCKQAAAFyBYAoAAABXIJgCAADAFQimAAAAcAWCKQAAAFyBYAoAAABXIJgCAADAFQimAAAAcAWCKQAAAFyBYAoAAABXIJgCAADAFQimAAAAcAWCKQAAAFyBYAoAAABXIJgCAADAFQimAAAAcAWCKQAAAFyBYAoAAABXIJgCAADAFQimAAAAcAVHg+n48eNVvXp1RUZGKjIyUnXr1tWCBQucLAkAAAAOcTSYli5dWiNHjtSaNWu0Zs0aNW3aVO3atdNPP/3kZFkAAABwQEEnN3777bf7zQ8bNkzjx4/XihUrVLVqVYeqAgAAgBMcDaZ/dubMGX300UdKTU1V3bp1s+yTlpamtLQ033xKSkp+lQcAAIDLzPGbn3744QeFh4fL6/WqR48emjNnjqpUqZJl3xEjRigqKso3xcXF5XO1AAAAuFwcD6YVK1bUhg0btGLFCj388MPq0qWLNm3alGXfQYMGKTk52TclJSXlc7UAAAC4XBz/KD8oKEjly5eXJNWqVUurV6/Wq6++qrfeeitTX6/XK6/Xm98lAgAAIB84fsb0XGbmdx0pAAAArg6OnjF98skn1apVK8XFxenYsWOaMWOGli5dqoULFzpZFgAAABzgaDD95ZdfdP/992v//v2KiopS9erVtXDhQjVv3tzJsgAAAOAAR4PpxIkTndw8AAAAXMR115gCAADg6kQwBQAAgCsQTAEAAOAKBFMAAAC4AsEUAAAArkAwBQAAgCsQTAEAAOAKBFMAAAC4AsEUAAAArkAwBQAAgCsQTAEAAOAKBFMAAAC4AsEUAAAArkAwBQAAgCsQTAEAAOAKBFMAAAC4AsEUAAAArkAwBQAAgCsQTAEAAOAKBFMAAAC4AsEUAAAArkAwBQAAgCsQTAEAAOAKBFMAAAC4AsEUAAAArkAwBQAAgCsQTAEAAOAKBFMAAAC4AsEUAAAArkAwBQAAgCsQTAEAAOAKBFMAAAC4gqPBdMSIEapdu7YiIiJUtGhR3XHHHdq6dauTJQEAAMAhjgbTZcuWqVevXlqxYoUWL16s9PR0tWjRQqmpqU6WBQAAAAcUdHLjCxcu9JufNGmSihYtqrVr1+qWW25xqCoAAAA4wdFgeq7k5GRJUnR0dJbL09LSlJaW5ptPSUnJl7oAAABw+bnm5iczU79+/dSgQQNdd911WfYZMWKEoqKifFNcXFw+VwkAAIDLxTXB9JFHHtHGjRs1ffr08/YZNGiQkpOTfVNSUlI+VggAAIDLyRUf5T/66KOaO3euvvrqK5UuXfq8/bxer7xebz5WBgAAgPziaDA1Mz366KOaM2eOli5dqrJlyzpZDgAAABzkaDDt1auXPvjgA/373/9WRESEDhw4IEmKiopSSEiIk6UBAAAgnzl6jen48eOVnJysxo0bq0SJEr5p5syZTpYFAAAABzj+UT4AAAAgueiufAAAAFzdCKYAAABwBYIpAAAAXIFgCgAAAFcgmAIAAMAVCKYAAABwBYIpAAAAXIFgCgAAAFcgmAIAAMAVCKYAAABwBYIpAAAAXIFgCgAAAFcgmAIAAMAVCKYAAABwBYIpAAAAXIFgCgAAAFcgmAIAAMAVCKYAAABwBYIpAAAAXIFgCgAAAFcgmAIAAMAVCKYAAABwBYIpAAAAXIFgCgAAAFcgmAIAAMAVCKYAAABwBYIpAAAAXIFgCgAAAFcgmAIAAMAVCKYAAABwBYIpAAAAXIFgCgAAAFdwNJh+9dVXuv3221WyZEl5PB598sknTpYDAAAABzkaTFNTU3X99ddr3LhxTpYBAAAAFyjo5MZbtWqlVq1aOVkCAAAAXMLRYHqx0tLSlJaW5ptPSUlxsBoAAADkpSvq5qcRI0YoKirKN8XFxTldEgAAAPLIFRVMBw0apOTkZN+UlJTkdEkAAADII1fUR/ler1der9fpMgAAAHAZXFFnTAEAAPDX5egZ0+PHj2vHjh2++d27d2vDhg2Kjo5WmTJlHKwMAAAA+c3RYLpmzRo1adLEN9+vXz9JUpcuXTR58mSHqgIAAIATHA2mjRs3lpk5WQIAAABcgmtMAQAA4AoEUwAAALgCwRQAAACuQDAFAACAKxBMAQAA4AoEUwAAALgCwRQAAACuQDAFAACAKxBMAQAA4AoEUwAAALgCwRQAAACuQDAFAACAKxBMAQAA4AoEUwAAALgCwRQAAACuQDAFAACAKxBMAQAA4AoEUwAAALgCwRQAAACuQDAFAACAKxBMAQAA4AoEUwAAALgCwRQAAACuQDAFAACAKxBMAQAA4AoEUwAAALgCwRQAAACuQDAFAACAKxBMAQAA4AoEUwAAALgCwRQAAACuQDAFAACAKzgeTN944w2VLVtWwcHBuvHGG/X11187XRIAAAAc4GgwnTlzpvr06aPBgwdr/fr1atiwoVq1aqXExEQnywIAAIADHA2mY8aM0YMPPqju3burcuXKGjt2rOLi4jR+/HgnywIAAIADCjq14VOnTmnt2rUaOHCgX3uLFi303XffZfmYtLQ0paWl+eaTk5MlSSkpKZev0HNkpP2eb9ty0iWNaZrlXSFudQnjc+bEmTwsxL1y+x46fobxyc6JU6l5XIk7Xcox6OTp03lYiTtdyvgcS+M9lJ0/54y/svzMTme3ZZaDfGAO2bdvn0myb7/91q992LBhVqFChSwfM3ToUJPExMTExMTExMR0hU1JSUkXzIeOnTE9y+Px+M2bWaa2swYNGqR+/fr55jMyMvTbb78pJibmvI+50qWkpCguLk5JSUmKjIx0uhxXYoyyx/hkj/HJHuNzYYxR9hif7F0N42NmOnbsmEqWLHnBvo4F0yJFiiggIEAHDhzwaz948KCKFSuW5WO8Xq+8Xq9fW6FChS5Xia4SGRn5l33D5hXGKHuMT/YYn+wxPhfGGGWP8cneX318oqKictTPsZufgoKCdOONN2rx4sV+7YsXL1a9evUcqgoAAABOcfSj/H79+un+++9XrVq1VLduXb399ttKTExUjx49nCwLAAAADnA0mLZv316HDx/Wc889p/379+u6667T/PnzFR8f72RZruL1ejV06NBMlzDgfxij7DE+2WN8ssf4XBhjlD3GJ3uMjz+PWU7u3QcAAAAuL8f/JCkAAAAgEUwBAADgEgRTAAAAuALBFHBI48aN1adPH6fLcD3G6dJNnjzZ1d/53LVrV91xxx1Ol+EnP953e/bskcfj0YYNGy7rdq5mbnxvIXsE0yvIM888oxo1auT68UuXLpXH49HRo0fzrCY38Xg8+uSTT3L9+PwOQB9//LGef/55SVJCQoLGjh2bp+u/1AOy28PM5cA+lr0TJ06ocOHCio6O1okTJ5wuJ8cu1759NlhmNz3zzDN5vt3ccPoXvMmTJ19wrJYuXepYfW5xqa/TpR7D3MDxP0kKXK2io6OdLgFXgdOnT+fZumbPnq3rrrtOZqaPP/5Y99133wW3HRgYmGfbd5u4uDjt37/fN//yyy9r4cKFWrJkia8tPDzcidJcp3379rrtttt883/729903XXX6bnnnvO1cUyExBnTi2Jmeumll3TNNdcoJCRE119/vWbNmiXpf2dKPv/8c9WqVUuhoaGqV6+etm7d6reOF154QUWLFlVERIS6d++ugQMH5tlvN9OmTVOtWrUUERGh4sWLq2PHjjp48KCkP36zb9KkiSSpcOHC8ng86tq1q6Q/fkPr3bu3nnjiCUVHR6t48eK5/i1/1qxZqlatmkJCQhQTE6Nbb71VqampvrN3zz77rIoWLarIyEg99NBDOnXqlO+xCxcuVIMGDVSoUCHFxMSobdu22rlz5yWNyVmHDx9Whw4dVLp0aYWGhqpatWqaPn26b3nXrl21bNkyvfrqq77f3vfs2ZPj1zU3zv5m3LhxY+3du1d9+/b1bTsnNV+qMWPGqFq1agoLC1NcXJx69uyp48ePS/rj/fzAAw8oOTk505mfhIQEDR8+XN26dVNERITKlCmjt99+O09qSk1NVefOnRUeHq4SJUpo9OjRfstPnTqlJ554QqVKlVJYWJhuvvlmv7MsWZ0tGDt2rBISEvKkPjfsY9ntJ2fP4H344Ydq3LixgoODNW3aNN9jP/nkE1WoUEHBwcFq3ry5kpKSLmrbEydOVKdOndSpUydNnDgx03KPx6M333xT7dq1U1hYmF544QWdOXNGDz74oMqWLauQkBBVrFhRr776apbrz+74kFvn27c3bdqk1q1bKzw8XMWKFdP999+vX3/99aLWHRAQoOLFi/um8PBwFSxY0K/to48+UuXKlRUcHKxKlSrpjTfeOO/6svqU4pNPPvEdE3IrqzHYuXPnBV+Xs8ft4cOHq1ixYipUqJCeffZZpaenq3///oqOjlbp0qX17rvvXrCGkJAQv3EJCgpSaGiob37Dhg1q2rTpeY//f35vN2zYUCEhIapdu7a2bdum1atXq1atWgoPD9dtt92mQ4cO5XqssjoG/fksZlafzBUqVEiTJ0+WJDVt2lSPPPKI3/LDhw/L6/Xqiy++yHVdZw0YMEAVKlRQaGiorrnmGg0ZMsT3y+fkyZP17LPP6vvvv/e9zmfr8ng8euedd3TnnXcqNDRU1157rebOnXvJ9VwWhhx78sknrVKlSrZw4ULbuXOnTZo0ybxery1dutS+/PJLk2Q333yzLV261H766Sdr2LCh1atXz/f4adOmWXBwsL377ru2detWe/bZZy0yMtKuv/76HG1/6NCh2fadOHGizZ8/33bu3GnLly+3OnXqWKtWrczMLD093WbPnm2SbOvWrbZ//347evSomZk1atTIIiMj7ZlnnrFt27bZlClTzOPx2KJFiy5qfP773/9awYIFbcyYMbZ7927buHGjvf7663bs2DHr0qWLhYeHW/v27e3HH3+0Tz/91GJjY+3JJ5/0PX7WrFk2e/Zs27Ztm61fv95uv/12q1atmp05cyZH25dkc+bMyXLZzz//bKNGjbL169fbzp077V//+pcFBATYihUrzMzs6NGjVrduXfv73/9u+/fvt/3791t6enqOXtfcatSokT322GN2+PBhK126tD333HO+beek5gvp0qWLtWvX7rzLX3nlFfviiy9s165d9vnnn1vFihXt4YcfNjOztLQ0Gzt2rEVGRvpqOnbsmJmZxcfHW3R0tL3++uu2fft2GzFihBUoUMA2b958aQNiZg8//LCVLl3aFi1aZBs3brS2bdtaeHi4PfbYY2Zm1rFjR6tXr5599dVXtmPHDhs1apR5vV7btm2bmWW9j7zyyisWHx+fo+27fR8zy34/2b17t0myhIQEmz17tu3atcv27dtnkyZNssDAQKtVq5Z99913tmbNGrvpppsu6n28Y8cO83q99ttvv9nhw4fN6/Xazp07/fpIsqJFi9rEiRNt586dtmfPHjt16pQ9/fTTtmrVKtu1a5dNmzbNQkNDbebMmb7H5eT4kFtZ7ds///yzFSlSxAYNGmSbN2+2devWWfPmza1Jkya+x53dPy/Gue+ft99+20qUKOF7LWbPnm3R0dE2efJkMzPf67V+/XozM5s0aZJFRUX5rXPOnDl2qT+qsxqDkydP5uh1iYiIsF69etmWLVts4sSJJslatmxpw4YNs23bttnzzz9vgYGBlpiYeFE1nTu+Fzr+nx2rsz+DN23aZHXq1LGaNWta48aN7ZtvvrF169ZZ+fLlrUePHn7PIbvj4LkudAzK6udMVFSUTZo0yczM3n//fStcuLCdPHnSt/zVV1+1hIQEy8jIuOhxOdfzzz9v3377re3evdvmzp1rxYoVsxdffNHMzH7//Xd7/PHHrWrVqr7X+ffff/fVXbp0afvggw9s+/bt1rt3bwsPD7fDhw/neGzyC8E0h44fP27BwcH23Xff+bU/+OCD1qFDB1+AWbJkiW/ZvHnzTJKdOHHCzMxuvvlm69Wrl9/j69evn2fB9FyrVq0ySb5AcbbGI0eO+PVr1KiRNWjQwK+tdu3aNmDAgBxvy8xs7dq1Jsn27NmTaVmXLl0sOjraUlNTfW3jx4+38PDw8wbPgwcPmiT74YcfcrT97IJpVlq3bm2PP/64bz6rA0JOXtfc+vP24uPj7ZVXXrnomrNzsQfkDz/80GJiYnzzWf2QNPuj1k6dOvnmMzIyrGjRojZ+/Pgcbysrx44ds6CgIJsxY4av7fDhwxYSEmKPPfaY7dixwzwej+3bt8/vcc2aNbNBgwaZ2eUPpufK730sK3/eT87+8B47dqxfn0mTJpkkv19qNm/ebJJs5cqVOdrOk08+aXfccYdvvl27djZ48GC/PpKsT58+F1xXz5497a677vLN5+b4cDHO3beHDBliLVq08OuTlJTk+6Uiq8fkxLnvn7i4OPvggw/8+jz//PNWt25dM8u/YGqWs+eT1esSHx/v9xpUrFjRGjZs6JtPT0+3sLAwmz59ep7Wc+7x/+xYvfPOO74+06dPN0n2+eef+9pGjBhhFStW9HsOOT0OXugYZHbhYHry5EmLjo72C/g1atSwZ555Jkc1XOz77qWXXrIbb7zRN3++Y5gke+qpp3zzx48fN4/HYwsWLMjxtvILH+Xn0KZNm3Ty5Ek1b95c4eHhvmnq1Kl+HzdUr17d9/8SJUpIku+jvq1bt+qmm27yW++585di/fr1ateuneLj4xUREaHGjRtLkhITEy/42D/XLf1R+9m6c+r6669Xs2bNVK1aNd1zzz2aMGGCjhw54rc8NDTUN1+3bl0dP37c93Hizp071bFjR11zzTWKjIxU2bJlc1z/hZw5c0bDhg1T9erVFRMTo/DwcC1atCjH687udb1cLrXmC/nyyy/VvHlzlSpVShEREercubMOHz6s1NTUCz72z+Ph8XhUvHjxSx6PnTt36tSpU6pbt66vLTo6WhUrVpQkrVu3TmamChUq+O2Dy5Yty7NLPi7E6X1Mytl+UqtWrUyPK1iwoF97pUqVVKhQIW3evPmC2zxz5oymTJmiTp06+do6deqkKVOm6MyZM359s9r2m2++qVq1aik2Nlbh4eGaMGFCpjG70PEhL61du1Zffvml3/uoUqVKkpRn76VDhw4pKSlJDz74oN92XnjhhXx7v15ITl6XqlWrqkCB/0WFYsWKqVq1ar75gIAAxcTE5Mn+n5Pj/5/3o2LFikmSXz3FihXLdS0XOgblhNfrVadOnXyXN2zYsEHff/+977KeSzVr1iw1aNDAd+nIkCFDcvVzLCwsTBEREZf951hucPNTDmVkZEiS5s2bp1KlSvkt83q9vgPNny/0P3td0NnH/rntLMujvwibmpqqFi1aqEWLFpo2bZpiY2OVmJioli1b5ug6rXNvUPB4PH5150RAQIAWL16s7777TosWLdJrr72mwYMHa+XKldk+7uyY3H777YqLi9OECRNUsmRJZWRk6LrrrsuT68xGjx6tV155RWPHjvVdV9mnT58cr/tCr+vlcKk1Z2fv3r1q3bq1evTooeeff17R0dH65ptv9OCDD+boZpm8eL+c60L7QkZGhgICArR27VoFBAT4LTt7g0mBAgUyrSevbv5xwz4m5Ww/CQsLy/KxWV2rmJPrFz/77DPt27dP7du392s/c+aMFi1apFatWp132x9++KH69u2r0aNHq27duoqIiNCoUaMueFy4mPouVkZGhm6//Xa9+OKLmZad/cUzL7YhSRMmTNDNN9/st+zc9+9Zl/P9e66cvi5ZvW8vx/6f0+N/Vsfic9tyW0tOfh57PJ4Lvkbdu3dXjRo19PPPP+vdd99Vs2bNFB8fn6ua/mzFihW699579eyzz6ply5aKiorSjBkzMl2Lfz6X43W7HAimOVSlShV5vV4lJiaqUaNGmZbn5DfgihUratWqVbr//vt9bWvWrMmT+rZs2aJff/1VI0eOVFxcXJbrDgoKkqRMZzjyksfjUf369VW/fn09/fTTio+P15w5cyRJ33//vU6cOKGQkBBJf+xk4eHhKl26tA4fPqzNmzfrrbfeUsOGDSVJ33zzTZ7V9fXXX6tdu3a+Mz4ZGRnavn27Kleu7OsTFBR0WccmO1ltOyc159aaNWuUnp6u0aNH+86GfPjhhxes6XIqX768AgMDtWLFCpUpU0aSdOTIEW3btk2NGjXSDTfcoDNnzujgwYO+98i5YmNjdeDAAZmZ74dWXn1HpBv2sUvZT9LT07VmzRrfpzRbt27V0aNHfWcKszNx4kTde++9Gjx4sF/7yJEjNXHiRL9geq6vv/5a9erVU8+ePX1tWR0vszs+XKpz38s1a9bU7NmzlZCQoIIFL8+PwWLFiqlUqVLatWvXBb+94KzY2FgdO3ZMqampvoCfV+/fc8cgp69Lfrjcx/+cutAxSPrjNfrzNzFs375dv//+u996qlWrplq1amnChAn64IMP9Nprr+VJfd9++63i4+P99sO9e/f69XHy51heIZjmUEREhP75z3+qb9++ysjIUIMGDZSSkqLvvvtO4eHhOfpt6NFHH9Xf//531apVS/Xq1dPMmTO1ceNGXXPNNTmu48SJE5kOVOHh4SpTpoyCgoL02muvqUePHvrxxx9935F5Vnx8vDwejz799FO1bt1aISEhefpVJitXrtTnn3+uFi1aqGjRolq5cqUOHTqkypUra+PGjTp16pQefPBBPfXUU9q7d6+GDh2qRx55RAUKFFDhwoUVExOjt99+WyVKlFBiYqIGDhx40TXs3r070/iUL19e5cuX1+zZs/Xdd9+pcOHCGjNmjA4cOOAX8hISErRy5Urt2bNH4eHh+frVJQkJCfrqq6907733yuv1qkiRIjmq+UKSk5MzjUd0dLTKlSun9PR0vfbaa7r99tv17bff6s0338xU0/Hjx/X555/7Pmb980eteS08PFwPPvig+vfvr5iYGBUrVkyDBw/2BecKFSrovvvuU+fOnTV69GjdcMMN+vXXX/XFF1+oWrVqat26tRo3bqxDhw7ppZde0t13362FCxdqwYIFioyMzHEdbt7HLmU/CQwM1KOPPqp//etfCgwM1COPPKI6depc8HKiQ4cO6T//+Y/mzp2r6667zm9Zly5d1KZNGx06dEixsbFZPr58+fKaOnWqPvvsM5UtW1bvvfeeVq9e7fuo9qzsjg+X6tx9u1evXpowYYI6dOig/v37q0iRItqxY4dmzJihCRMmnPeM5sV65pln1Lt3b0VGRqpVq1ZKS0vTmjVrdOTIEfXr1y9T/5tvvlmhoaF68skn9eijj2rVqlW+u6ov1bljkNPXJT/k1fH/Ul3oGCT9cdf9uHHjVKdOHWVkZGjAgAFZfiVa9+7d9cgjjyg0NFR33nnnRdVx6NChTMeg4sWLq3z58kpMTNSMGTNUu3ZtzZs3z3fi56yEhATfz8HSpUsrIiJCXq/3orbvOOcub73yZGRk2KuvvmoVK1a0wMBAi42NtZYtW9qyZcuyvOlh/fr1Jsl2797ta3vuueesSJEiFh4ebt26dbPevXtbnTp1crT9oUOHmqRMU6NGjczM7IMPPrCEhATzer1Wt25dmzt3rt+F9We3X7x4cfN4PNalSxczy/pi63bt2vmW59SmTZusZcuWFhsba16v1ypUqGCvvfaamf3vAvSnn37aYmJiLDw83Lp37+535+LixYutcuXK5vV6rXr16rZ06dKLuqEpq7GRZF9++aUdPnzY2rVrZ+Hh4Va0aFF76qmnrHPnzn4XxW/dutXq1KljISEhvtctp69rbvx53JcvX27Vq1c3r9fru9EhJzVnp0uXLlmOx9nXdcyYMVaiRAkLCQmxli1b2tSpUzM91x49elhMTIxJsqFDh5pZ1jdqXX/99b7ll+LYsWPWqVMnCw0NtWLFitlLL73kN05n7/BOSEiwwMBAK168uN155522ceNG3zrGjx9vcXFxFhYWZp07d7Zhw4Zd1M1Pbt7HzLLfT869measszfVzJ4926655hoLCgqypk2bZnmj4rlefvllK1SokJ06dSrTstOnT1t0dLSNHj3azLK+MeTkyZPWtWtXi4qKskKFCtnDDz9sAwcO9LtBIyfHh0uR1b69bds2u/POO61QoUIWEhJilSpVsj59+vjunM6Lm5/M/rhLu0aNGhYUFGSFCxe2W265xT7++GMzy3zzk9kfNzuVL1/egoODrW3btvb222/nyc1P547Bli1bcvy6/FlW45LTmzezW8+Fjv9ZjVVWx+dzbyC72JtAL3QM2rdvn7Vo0cLCwsLs2muvtfnz5/vd/PTn9YSGhlrPnj1zvG2zP8Ylq2PQ2eNr//79fftI+/bt7ZVXXvF7vidPnrS77rrLChUqZJJ8dWW1b2ZVtxt4zPLoIkfkSvPmzVW8eHG99957TpdyWXXt2lVHjx69pL/MBABAfmvcuLFq1KhxUX+dLykpSQkJCVq9erVq1qx5+Yr7C+Kj/Hz0+++/680331TLli0VEBCg6dOna8mSJVq8eLHTpQEAgEt0+vRp7d+/XwMHDlSdOnUIpbnA10XlI4/Ho/nz56thw4a68cYb9Z///EezZ8/WrbfeKkl+Xyly7vT11187XL2zhg8fft6xye7Gi7+qxMTEbN8vefWVUn817GNZq1q16nnH5f3333e6PEd9/fXX2b5v8D8cp/93g9LatWszXbfPeyln+CjfRXbs2HHeZaVKlfLdrXo1+u233/Tbb79luSwkJCTTV3j91aWnp2vPnj3nXX457za+krGPZW3v3r3n/VqiYsWKKSIiIp8rco8TJ05o3759511evnz5fKzG3ThOZ4/3Us4QTAEAAOAKfJQPAAAAVyCYAgAAwBUIpgAAAHAFgikAAABcgWAKAFlo3Lix+vTp43QZAHBVIZgCAADAFQimAAAAcAWCKQBcwLRp01SrVi1FRESoePHi6tixow4ePOhbvnTpUnk8Hn3++eeqVauWQkNDVa9ePW3dutVvPS+88IKKFi2qiIgIde/eXQMHDlSNGjV8y7O6fOCOO+5Q165dc1yLJM2dO1fXXnutQkJC1KRJE02ZMkUej0dHjx719fnuu+90yy23KCQkRHFxcerdu7dSU1MveawA4FIQTAHgAk6dOqXnn39e33//vT755BPt3r3bLyyeNXjwYI0ePVpr1qxRwYIF1a1bN9+y999/X8OGDdOLL76otWvXqkyZMho/fnye17Jnzx7dfffduuOOO7RhwwY99NBDGjx4sN86fvjhB7Vs2VJ/+9vftHHjRs2cOVPffPONHnnkkYuuBwDyEn/5CQCy0LhxY9WoUUNjx47NtGz16tW66aabdOzYMYWHh2vp0qVq0qSJlixZombNmkmS5s+frzZt2ujEiRMKDg5WnTp1VKtWLY0bN863ngYNGuj48ePasGHDebd5xx13qFChQpo8eXKWdZ5by8CBAzVv3jz98MMPvj5PPfWUhg0bpiNHjqhQoULq3LmzQkJC9NZbb/n6fPPNN2rUqJFSU1MVHByc+4EDgEvAGVMAuID169erXbt2io+PV0REhBo3bixJSkxM9OtXvXp13/9LlCghSb6P2bdu3aqbbrrJr/+583lRy9atW1W7du1st7N27VpNnjxZ4eHhvqlly5bKyMjQ7t27L7omAMgrBZ0uAADcLDU1VS1atFCLFi00bdo0xcbGKjExUS1bttSpU6f8+gYGBvr+7/F4JEkZGRmZ2s469wOrAgUKZGo7ffr0RdViZhfcTkZGhh566CH17t070/MtU6ZM1gMBAPmAYAoA2diyZYt+/fVXjRw5UnFxcZKkNWvWXPR6KlasqFWrVun+++/3tZ27ntjYWO3fv983f+bMGf34449q0qRJjmupVKmS5s+f79d2bp+aNWvqp59+Uvny5S/6eQDA5cRH+QCQjTJlyigoKEivvfaadu3apblz5+r555+/6PU8+uijmjhxoqZMmaLt27frhRde0MaNG/3ObjZt2lTz5s3TvHnztGXLFvXs2dPvTvqc1PLQQw9py5YtGjBggLZt26YPP/zQd33q2W0NGDBAy5cvV69evbRhwwZt375dc+fO1aOPPnrxAwQAeYhgCgDZiI2N1eTJk/XRRx+pSpUqGjlypF5++eWLXs99992nQYMG6Z///Kdq1qzpu5v+zzcadevWTV26dFHnzp3VqFEjlS1b1ne2NKe1lC1bVrNmzdLHH3+s6tWra/z48b678r1er6Q/roVdtmyZtm/froYNG+qGG27QkCFDfNfFAoBTuCsfABzSvHlzFS9eXO+9995l3c6wYcP05ptvKikp6bJuBwAuFdeYAkA++P333/Xmm2+qZcuWCggI0PTp07VkyRItXrw4z7f1xhtvqHbt2oqJidG3336rUaNG8R2lAK4IBFMAyAcej0fz58/XCy+8oLS0NFWsWFGzZ8/WrbfemufbOnsN62+//aYyZcro8ccf16BBg/J8OwCQ1/goHwAAAK7AzU8AAABwBYIpAAAAXIFgCgAAAFcgmAIAAMAVCKYAAABwBYIpAAAAXIFgCgAAAFcgmAIAAMAV/j+QuAWzxY1qRgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a figure\n",
    "fig, axes = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "# create a bar plot for each langauge\n",
    "# TODO: your code goes here\n",
    "for lang, loss_list in losses.items():\n",
    "    axes.bar(lang, sum(loss_list) / len(loss_list), label=lang)\n",
    "\n",
    "# format plot\n",
    "axes.set_xlabel(\"language\") # x-axis label\n",
    "axes.set_xticks(range(len(LANGUAGES))) # x-axis ticks\n",
    "axes.set_xticklabels(losses.keys()) # x-axis tick labels\n",
    "axes.set_ylabel(\"loss\") # y-axis label\n",
    "axes.set_ylim(0, 9) # range of y-axis\n",
    "axes.set_title(MODEL_NAME); # title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing XGLM to GPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to re-run the analysis above, but using `gpt2` as the pre-trained language model. For this exercise, focus on your native language, unless it's English or isn't covered by flores. In that case, pick another language that you can read well. \n",
    "\n",
    "Compare the language modeling loss of XGLM and GPT2. What do you observe? Investigate the differences in tokenization for XGLM and GPT2. What do you observe? How can the good (or bad) performance of GPT2 be explained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code goes here\n",
    "# Choosing hindi as the language\n",
    "\n",
    "hindi_data = load_dataset(\"facebook/flores\", \"hin_Deva\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_2_NAME = \"openai-community/gpt2\" # specify model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 1\n",
      "URL: https://en.wikinews.org/wiki/Scientists_say_new_medical_diagnostic_chip_can_sort_cells_anywhere_with_an_inkjet\n",
      "domain: wikinews\n",
      "topic: health\n",
      "has_image: 0\n",
      "has_hyperlink: 0\n",
      "sentence:  ,                            :                        .\n"
     ]
    }
   ],
   "source": [
    "for feature, value in hindi_data[\"dev\"][0].items():\n",
    "    print(f\"{feature}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b0060126f74d13a76496e01c47cd76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/997 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8960103db34b70907ef64d652a7e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize the data\n",
    "\n",
    "# load a pre-trained tokenizer from the huggingface hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_2_NAME)\n",
    "\n",
    "# gpt2 does not have a padding token, so we have to add it manually\n",
    "# if MODEL_2_NAME == \"gpt2\":\n",
    "tokenizer.add_special_tokens({'pad_token': tokenizer.unk_token})\n",
    "    # tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# specify the tokenization function\n",
    "def tokenization(example):\n",
    "    # fill in here\n",
    "    tokenized = tokenizer(example[\"sentence\"], return_tensors=\"pt\", padding='max_length', truncation=True)\n",
    "    return tokenized\n",
    "    pass\n",
    "\n",
    "# TODO: your code goes here\n",
    "tokenized_data_hindi = hindi_data.map(tokenization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 1\n",
      "URL: https://en.wikinews.org/wiki/Scientists_say_new_medical_diagnostic_chip_can_sort_cells_anywhere_with_an_inkjet\n",
      "domain: wikinews\n",
      "topic: health\n",
      "has_image: 0\n",
      "has_hyperlink: 0\n",
      "sentence:  ,                            :                        .\n",
      "input_ids: [[11976, 116, 24231, 233, 11976, 106, 11976, 113, 48077, 11976, 108, 28225, 243, 24231, 233, 11, 28225, 116, 24231, 235, 11976, 253, 24231, 230, 11976, 101, 11976, 104, 11976, 120, 24231, 233, 11976, 108, 24231, 235, 11976, 94, 28225, 107, 24231, 224, 11976, 101, 11976, 123, 11976, 113, 11976, 108, 24231, 235, 11976, 116, 11976, 123, 11976, 253, 24231, 222, 28225, 116, 24231, 235, 11976, 243, 24231, 224, 11976, 110, 28225, 239, 11976, 104, 11976, 120, 28225, 106, 24231, 229, 11976, 94, 11976, 123, 11976, 116, 11976, 123, 11976, 101, 28225, 243, 24231, 229, 28225, 113, 24231, 230, 11976, 250, 24231, 235, 11976, 252, 48077, 11976, 101, 11976, 123, 11976, 243, 24231, 233, 11976, 224, 28225, 101, 24231, 229, 28225, 237, 11976, 243, 28225, 101, 11976, 237, 28225, 94, 48077, 11976, 107, 11976, 245, 24231, 235, 11976, 101, 24231, 233, 11976, 116, 24231, 235, 11976, 253, 11976, 123, 11976, 243, 28225, 231, 11976, 103, 11976, 243, 11976, 108, 11976, 96, 28225, 243, 24231, 229, 28225, 228, 11976, 113, 11976, 123, 11976, 115, 24231, 235, 11976, 243, 48077, 11976, 108, 28225, 243, 24231, 222, 28225, 246, 24231, 233, 11976, 115, 11976, 96, 48077, 28225, 243, 24231, 222, 28225, 250, 24231, 233, 28225, 243, 24231, 233, 11976, 114, 11976, 123, 11976, 243, 48077, 11976, 241, 11976, 224, 28225, 243, 24231, 233, 28225, 231, 11976, 101, 11976, 243, 24231, 229, 28225, 103, 24231, 235, 11976, 108, 11976, 243, 48077, 11976, 108, 28225, 243, 24231, 229, 28225, 228, 11976, 100, 48077, 11976, 108, 28225, 103, 11976, 108, 28225, 249, 48077, 11976, 223, 11976, 253, 28225, 116, 11976, 243, 11976, 97, 48077, 28225, 117, 24231, 230, 25, 28225, 237, 11976, 243, 28225, 249, 24231, 233, 11976, 253, 24231, 222, 28225, 103, 24231, 235, 11976, 108, 11976, 123, 11976, 224, 11976, 253, 28225, 243, 11976, 108, 11976, 101, 24231, 229, 28225, 107, 24231, 233, 11976, 245, 24231, 235, 11976, 107, 28225, 248, 11976, 123, 11976, 103, 28225, 250, 11976, 123, 11976, 116, 24231, 229, 28225, 116, 24231, 235, 11976, 253, 24231, 230, 11976, 96, 24231, 235, 11976, 94, 11976, 108, 24231, 235, 11976, 94, 28225, 229, 11976, 224, 11976, 243, 11976, 250, 24231, 229, 11976, 253, 28225, 103, 24231, 235, 11976, 108, 11976, 123, 11976, 224, 11976, 253, 11976, 108, 28225, 243, 48077, 28225, 231, 11976, 103, 11976, 107, 24231, 233, 11976, 245, 28225, 243, 11976, 108, 11976, 243, 24231, 229, 28225, 110, 11976, 245, 11976, 255, 11976, 245, 28225, 237, 11976, 243, 28225, 227, 11976, 106, 24231, 229, 11976, 108, 11976, 123, 11976, 243, 24231, 222, 28225, 116, 24231, 229, 11976, 224, 11976, 253, 28225, 243, 24231, 229, 28225, 110, 11976, 123, 11976, 237, 28225, 101, 11976, 123, 11976, 108, 24231, 235, 11976, 106, 11976, 123, 11976, 97, 28225, 243, 11976, 123, 11976, 107, 48077, 28225, 250, 48077, 28225, 116, 11976, 243, 11976, 97, 48077, 28225, 117, 24231, 230, 13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]\n",
      "attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# let's take a look at a tokenized sample\n",
    "# TODO: your code goes here\n",
    "for feature, value in tokenized_data_hindi[\"dev\"][0].items():\n",
    "    print(f\"{feature}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data_hindi_dev = [filter_keys(entry) for entry in tokenized_data_hindi[\"dev\"]]\n",
    "filtered_data_hindi_devtest = [filter_keys(entry) for entry in tokenized_data_hindi[\"devtest\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [[11976, 116, 24231, 233, 11976, 106, 11976, 113, 48077, 11976, 108, 28225, 243, 24231, 233, 11, 28225, 116, 24231, 235, 11976, 253, 24231, 230, 11976, 101, 11976, 104, 11976, 120, 24231, 233, 11976, 108, 24231, 235, 11976, 94, 28225, 107, 24231, 224, 11976, 101, 11976, 123, 11976, 113, 11976, 108, 24231, 235, 11976, 116, 11976, 123, 11976, 253, 24231, 222, 28225, 116, 24231, 235, 11976, 243, 24231, 224, 11976, 110, 28225, 239, 11976, 104, 11976, 120, 28225, 106, 24231, 229, 11976, 94, 11976, 123, 11976, 116, 11976, 123, 11976, 101, 28225, 243, 24231, 229, 28225, 113, 24231, 230, 11976, 250, 24231, 235, 11976, 252, 48077, 11976, 101, 11976, 123, 11976, 243, 24231, 233, 11976, 224, 28225, 101, 24231, 229, 28225, 237, 11976, 243, 28225, 101, 11976, 237, 28225, 94, 48077, 11976, 107, 11976, 245, 24231, 235, 11976, 101, 24231, 233, 11976, 116, 24231, 235, 11976, 253, 11976, 123, 11976, 243, 28225, 231, 11976, 103, 11976, 243, 11976, 108, 11976, 96, 28225, 243, 24231, 229, 28225, 228, 11976, 113, 11976, 123, 11976, 115, 24231, 235, 11976, 243, 48077, 11976, 108, 28225, 243, 24231, 222, 28225, 246, 24231, 233, 11976, 115, 11976, 96, 48077, 28225, 243, 24231, 222, 28225, 250, 24231, 233, 28225, 243, 24231, 233, 11976, 114, 11976, 123, 11976, 243, 48077, 11976, 241, 11976, 224, 28225, 243, 24231, 233, 28225, 231, 11976, 101, 11976, 243, 24231, 229, 28225, 103, 24231, 235, 11976, 108, 11976, 243, 48077, 11976, 108, 28225, 243, 24231, 229, 28225, 228, 11976, 100, 48077, 11976, 108, 28225, 103, 11976, 108, 28225, 249, 48077, 11976, 223, 11976, 253, 28225, 116, 11976, 243, 11976, 97, 48077, 28225, 117, 24231, 230, 25, 28225, 237, 11976, 243, 28225, 249, 24231, 233, 11976, 253, 24231, 222, 28225, 103, 24231, 235, 11976, 108, 11976, 123, 11976, 224, 11976, 253, 28225, 243, 11976, 108, 11976, 101, 24231, 229, 28225, 107, 24231, 233, 11976, 245, 24231, 235, 11976, 107, 28225, 248, 11976, 123, 11976, 103, 28225, 250, 11976, 123, 11976, 116, 24231, 229, 28225, 116, 24231, 235, 11976, 253, 24231, 230, 11976, 96, 24231, 235, 11976, 94, 11976, 108, 24231, 235, 11976, 94, 28225, 229, 11976, 224, 11976, 243, 11976, 250, 24231, 229, 11976, 253, 28225, 103, 24231, 235, 11976, 108, 11976, 123, 11976, 224, 11976, 253, 11976, 108, 28225, 243, 48077, 28225, 231, 11976, 103, 11976, 107, 24231, 233, 11976, 245, 28225, 243, 11976, 108, 11976, 243, 24231, 229, 28225, 110, 11976, 245, 11976, 255, 11976, 245, 28225, 237, 11976, 243, 28225, 227, 11976, 106, 24231, 229, 11976, 108, 11976, 123, 11976, 243, 24231, 222, 28225, 116, 24231, 229, 11976, 224, 11976, 253, 28225, 243, 24231, 229, 28225, 110, 11976, 123, 11976, 237, 28225, 101, 11976, 123, 11976, 108, 24231, 235, 11976, 106, 11976, 123, 11976, 97, 28225, 243, 11976, 123, 11976, 107, 48077, 28225, 250, 48077, 28225, 116, 11976, 243, 11976, 97, 48077, 28225, 117, 24231, 230, 13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]\n",
      "attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "for feature, value in filtered_data_hindi_dev[0].items():\n",
    "    print(f\"{feature}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_hindi_dev = torch.utils.data.DataLoader(filtered_data_hindi_dev, batch_size=BATCH_SIZE)\n",
    "data_loader_hindi_devtest = torch.utils.data.DataLoader(filtered_data_hindi_devtest, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7355e03c7f849669da65ca3baf1695e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = GPT2Model.from_pretrained(MODEL_2_NAME)\n",
    "model_2.eval()\n",
    "# model_2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/devanshsrivastav/Documents/NNTI_Assignments/NNTIProjectFiles/notebooks/task1.ipynb Cell 38\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/devanshsrivastav/Documents/NNTI_Assignments/NNTIProjectFiles/notebooks/task1.ipynb#X50sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Compute model outputs\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/devanshsrivastav/Documents/NNTI_Assignments/NNTIProjectFiles/notebooks/task1.ipynb#X50sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/devanshsrivastav/Documents/NNTI_Assignments/NNTIProjectFiles/notebooks/task1.ipynb#X50sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model_2(input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/devanshsrivastav/Documents/NNTI_Assignments/NNTIProjectFiles/notebooks/task1.ipynb#X50sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Get logits for next word prediction\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/devanshsrivastav/Documents/NNTI_Assignments/NNTIProjectFiles/notebooks/task1.ipynb#X50sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits[:, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mcontiguous()  \u001b[39m# Discard the last token's logits\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dev/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:844\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    843\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwte(input_ids)\n\u001b[0;32m--> 844\u001b[0m position_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwpe(position_ids)\n\u001b[1;32m    845\u001b[0m hidden_states \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39m position_embeds\n\u001b[1;32m    847\u001b[0m \u001b[39mif\u001b[39;00m token_type_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dev/lib/python3.9/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dev/lib/python3.9/site-packages/torch/nn/functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2230\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2233\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "losses_hindi = []\n",
    "for batch in data_loader_hindi_dev:\n",
    "    # Extract input IDs and attention masks\n",
    "    input_ids = torch.stack([torch.cat(seq) for seq in batch['input_ids']])\n",
    "    attention_mask = torch.stack([torch.cat(seq) for seq in batch['attention_mask']])\n",
    "    \n",
    "    # Shift input IDs to create target sequence\n",
    "    target_ids = input_ids[:, 1:].contiguous()  # Shift by one token\n",
    "    target_mask = attention_mask[:, 1:].contiguous()\n",
    "    \n",
    "    # Compute model outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model_2(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # Get logits for next word prediction\n",
    "    logits = outputs.logits[:, :-1].contiguous()  # Discard the last token's logits\n",
    "    \n",
    "    # Flatten the logits and target sequences\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    target_ids_flat = target_ids.view(-1)\n",
    "    target_mask_flat = target_mask.view(-1)\n",
    "    \n",
    "    # Filter out masked positions (where target_mask_flat == 0)\n",
    "    masked_indices = target_mask_flat.nonzero().squeeze()\n",
    "    logits_flat_masked = logits_flat.index_select(0, masked_indices)\n",
    "    target_ids_flat_masked = target_ids_flat.index_select(0, masked_indices)\n",
    "    \n",
    "    # Compute cross-entropy loss\n",
    "    loss = torch.nn.functional.cross_entropy(logits_flat_masked, target_ids_flat_masked)\n",
    "    \n",
    "    # Append loss to the list for the current language\n",
    "    losses_hindi.append(loss.item())\n",
    "print(f\"Completed loss computation for Hindi\")\n",
    "print(f\"Average loss for Hindi: {np.mean(losses_hindi)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
